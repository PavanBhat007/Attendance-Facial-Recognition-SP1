{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Please refer to [this](https://www.youtube.com/watch?v=GRtgLlwxpc4) video before running**"
      ],
      "metadata": {
        "id": "cOmEvOol0HvJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLO V5**"
      ],
      "metadata": {
        "id": "yB-l6lsI0X6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Initial setup and config**"
      ],
      "metadata": {
        "id": "0DUtHXp90boT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYFZFXjfyEfi"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How to train on custom dataset**\n",
        "\n",
        "- Make a folder, say `custom_data` which must contain the folders `images` and `labels` (images and labels must be the exact names of the folders within custom_data).\n",
        "  - images folder will have 2 folders train and val containing images for training and validation.\n",
        "  - labels folder will be empty as of now\n",
        "\n",
        "  <br><hr><br>\n",
        "- Next we must label the images to fill up the labels folder and for this we will make use of [Makesense](https://www.makesense.ai).\n",
        "  - Upload train folder in images, and give the label name as `person` (we are using only one class label `person` for this instruction/ tutorial)\n",
        "  - Next for each of the images, draw bounding boxes and choose the label - person\n",
        "  - Next click on `Actions` tab and select `Export annotations` and then select `a 'zip' package containing files in YOLO format.`\n",
        "  - Extract the downloaded zip file into labels (train)\n",
        "  - Repeat same process for the val folder in images too.\n",
        "- Now the labels folder will have train and val folders containing labels with the same name as the images.\n",
        "\n",
        "  <br><hr><br>\n",
        "\n",
        "- Now our custom data is ready and we have to tweak some things in the default YOLO implementation to run it on our custom data.\n",
        "- First, compress the `custom_data` folder and upload the `custom_data.zip` folder to colab. Then extract it.\n",
        "- To unzip the file create a new code cell before train and insert the following and run the cell : <br> `!unzip -q ../custom_data.zip -d ../` <br>\n",
        "- Then, go to the data folder and search for `coc128.yaml` file and download it.\n",
        "  - In the `coco128.yaml` file, change `nc=1` and `names=[\"person\"]`. Also change the file paths for train and val as per the `custom_data` folder in google colab. (which will be `../custom_data/images/train` and `../custom_data/images/val` as per the above instructions.\n",
        "  - Save the file as, say `custom.yaml` and upload it back to the data folder of YOLO in colab.\n",
        "\n",
        "  <br><hr><br>\n",
        "\n",
        "- Now the base setup is done and with last few changes the model will be ready.\n",
        "- In the training codeblock, change `--data coc128.yaml` to `--data custom.yaml` (as per above instructions). Other things like `--batch` and `--epochs` can be tweaked to get weights with good accuracy.\n",
        "- Run the train cell.\n",
        "- The last epoch in the output will have the name of the weights and accuracy obtained on it. Weights will be given in the last second line in the output.\n",
        "\n",
        "- Next, in the detect codeblock, change the `--weights yolov5s.pt` to whatever weights we got in the output of the train codeblock, and the `--source data/images` to `--source text.mp4` (dataset for testing , can be video or images). Results are saved in `runs/detect` which can be downloaded and viewed later.\n",
        "\n",
        "<br><hr><br>\n",
        "#### The above instructions are for training YOLO on custom dataset for just one person class. If we want to include more people, then the changes would be as follows : <br> \n",
        "- In the `coco128.yaml` file, change `nc = (number of person labels)` and `names=[\"person1\", \"person2\", \"person3\", ...]`.\n",
        "<br><br>\n",
        "For example, if we have to detect 4 people in a video, then `nc = 4` and `names = [\"Nuthan\", \"Pragathi\", \"Ankitha\", \"Pavan\"]`\n"
      ],
      "metadata": {
        "id": "-SXYUmjv0vb2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "## 1. Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources, downloading models automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases), and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image \n",
        "                          vid.mp4  # video\n",
        "                          screen  # screenshot\n",
        "                          path/  # directory\n",
        "                         'path/*.jpg'  # glob\n",
        "                         'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "metadata": {
        "id": "0GDgllB9yknM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "## 2. Train\n",
        "\n",
        "YOLOv5s model is default trained on the [COCO128](https://www.kaggle.com/ultralytics/coco128) dataset with `--data coco128.yaml`, starting from pretrained `--weights yolov5s.pt`, or from randomly initialized `--weights '' --cfg yolov5s.yaml`.\n",
        "\n",
        "We can get the custom dataset and upload it in data folder and after training we can take the custom weights obtained and put it in the above cell in place of  `--weights yolov5s.pt`\n",
        "\n",
        "- **Training Results** are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n",
        "<br>\n",
        "\n",
        "## Label a dataset on [Makesense](https://www.makesense.ai)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "id": "vv_TAyBnyqZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}